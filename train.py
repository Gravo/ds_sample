import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from transformers import Qwen2Tokenizer, get_scheduler
from tqdm.auto import tqdm
import os

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data.dataset import AddressTokenDataset
from model.address_analyzer import AddressModelConfig, AddressAnalyzerModel

# ===================== 1. è®­ç»ƒé…ç½® =====================
class TrainConfig:
    # æ•°æ®è·¯å¾„
    tokenizer_dir = "./qwen3_address_5w_tokenizer_final"
    data_path = "./annotated_brazil_address.jsonl"
    save_dir = "./trained_model"
    # è®­ç»ƒè¶…å‚æ•°
    batch_size = 32  # 6GB GPUå¯è®¾32ï¼Œ12GBå¯è®¾64
    max_len = 64
    epochs = 10
    lr = 2e-5
    weight_decay = 1e-5
    # ç¡¬ä»¶é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # æ—¥å¿—é…ç½®
    log_step = 100  # æ¯100æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—

# ===================== 2. è®­ç»ƒå‡½æ•° =====================
def train():
    cfg = TrainConfig()
    os.makedirs(cfg.save_dir, exist_ok=True)

    # æ­¥éª¤1ï¼šåŠ è½½Tokenizer
    tokenizer = Qwen2Tokenizer.from_pretrained(cfg.tokenizer_dir, local_files_only=True)
    tokenizer.pad_token = tokenizer.eos_token
    print(f"âœ… åŠ è½½Qwen2Tokenizerå®Œæˆï¼Œè¯è¡¨å¤§å°ï¼š{tokenizer.vocab_size}")

    # æ­¥éª¤2ï¼šåŠ è½½æ•°æ®é›†å¹¶æ‹†åˆ†è®­ç»ƒ/éªŒè¯ï¼ˆ8:2ï¼‰
    full_dataset = AddressTokenDataset(cfg.data_path, tokenizer, cfg.max_len)
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼šè®­ç»ƒé›†{len(train_dataset)}æ¡ï¼ŒéªŒè¯é›†{len(val_dataset)}æ¡")

    # æ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False)

    # æ­¥éª¤3ï¼šåˆå§‹åŒ–æ¨¡å‹ï¼ˆé€‚é…3000ä¸‡å‚æ•°ï¼‰
    model_config = AddressModelConfig(
        vocab_size=tokenizer.vocab_size,
        d_model=512,
        n_layers=6,
        n_heads=8,
        d_ff=2048,
        pad_token_id=tokenizer.pad_token_id,
        num_labels=6
    )
    model = AddressAnalyzerModel(model_config).to(cfg.device)
    print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œæ€»å‚æ•°ï¼š{sum(p.numel() for p in model.parameters())/1e6:.2f} ä¸‡")
    print(f"âœ… è®­ç»ƒè®¾å¤‡ï¼š{cfg.device}")

    # æ­¥éª¤4ï¼šé…ç½®ä¼˜åŒ–å™¨&å­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=cfg.lr,
        weight_decay=cfg.weight_decay
    )
    # çº¿æ€§å­¦ä¹ ç‡è¡°å‡
    num_training_steps = cfg.epochs * len(train_dataloader)
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps
    )

    # æ­¥éª¤5ï¼šè®­ç»ƒå¾ªç¯
    best_val_loss = float("inf")
    progress_bar = tqdm(range(num_training_steps), desc="Training")

    for epoch in range(cfg.epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for step, batch in enumerate(train_dataloader):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            batch = {k: v.to(cfg.device) for k, v in batch.items()}
            # å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                labels=batch["labels"]
            )
            loss = outputs["loss"]
            train_loss += loss.item() * batch["input_ids"].size(0)
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            # æ—¥å¿—
            progress_bar.update(1)
            if step % cfg.log_step == 0 and step > 0:
                tqdm.write(f"Epoch {epoch+1}/{cfg.epochs} | Step {step} | Train Loss: {loss.item():.4f}")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in val_dataloader:
                batch = {k: v.to(cfg.device) for k, v in batch.items()}
                outputs = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"]
                )
                val_loss += outputs["loss"].item() * batch["input_ids"].size(0)

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_loss = train_loss / len(train_dataset)
        avg_val_loss = val_loss / len(val_dataset)

        # æ‰“å°epochæ—¥å¿—
        tqdm.write("="*80)
        tqdm.write(f"Epoch {epoch+1}/{cfg.epochs} Summary")
        tqdm.write(f"Average Train Loss: {avg_train_loss:.4f}")
        tqdm.write(f"Average Val Loss: {avg_val_loss:.4f}")
        tqdm.write("="*80)

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            model.save_pretrained(cfg.save_dir)
            tokenizer.save_pretrained(cfg.save_dir)
            tqdm.write(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼‰åˆ° {cfg.save_dir}")

    # è®­ç»ƒå®Œæˆ
    progress_bar.close()
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä¼˜éªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼Œæ¨¡å‹ä¿å­˜è·¯å¾„ï¼š{cfg.save_dir}")

# ===================== 3. è¿è¡Œè®­ç»ƒ =====================
if __name__ == "__main__":
    train()